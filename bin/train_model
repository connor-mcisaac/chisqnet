#! /usr/bin/env python

import argparse
import logging, time
import h5py
import numpy as np
from pycbc import waveform
from pycbc.types import zeros, float32, complex64
from chisqnet.filtering import ChisqFilter, PolyShiftTransform
import tensorflow as tf
import tensorflow_probability as tfp
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from chisqnet.training_utils import SampleFile, chi2, nc_chi2


parser = argparse.ArgumentParser()

# Gather inputs
parser.add_argument("--sample-file", required=True,
                    help="Sample file to be used in training")
parser.add_argument("--checkpoint-file",
                    help="Checkpoint file with the latest state of the "
                         "model to be laoded")

# Gather arguments for filtering/bank
parser.add_argument("--bank-file", required=True,
                    help="The bank file to be used to generate templates")
parser.add_argument("--low-frequency-cutoff", type=float,
                  help="The low frequency cutoff to use for filtering (Hz)")
parser.add_argument("--enable-bank-start-frequency", action='store_true',
                  help="Read the starting frequency of template waveforms"
                       " from the template bank.")
parser.add_argument("--max-template-length", type=float,
                  help="The maximum length of a template is seconds. The "
                       "starting frequency of the template is modified to "
                       "ensure the proper length")
waveform.bank.add_approximant_arg(parser)
parser.add_argument("--order", type=int,
                  help="The integer half-PN order at which to generate"
                       " the approximant. Default is -1 which indicates to use"
                       " approximant defined default.", default=-1,
                       choices = np.arange(-1, 9, 1))
taper_choices = ["start","end","startend"]
parser.add_argument("--taper-template", choices=taper_choices,
                    help="For time-domain approximants, taper the start and/or"
                         " end of the waveform before FFTing.")

# Gather arguments for training
parser.add_argument("--batch-size", required=True, type=int,
                    help="The number of samples to analyse in one batch")
parser.add_argument("--epochs", required=True, type=int,
                     help="The number of times to analyse the full dataset")
parser.add_argument("--shuffle", action="store_true",
                    help="If given, shuffle the order of analysed segments each epoch")
parser.add_argument("--snr-cut-width", default=0.1, type=float,
                    help="The width around each trigger to cut from the SNR timeseries")
parser.add_argument("--learning-rate", default=0.001, type=float,
                    help="The learning rate passed to the training optimizer")

# Gather arguments for shift transform
parser.add_argument("--shift-num", default=8, type=int,
                    help="The number of shift transformations to train")
parser.add_argument("--poly-degree", default=2, type=int,
                    help="The degree of the polynomial use in the shift transformations")
parser.add_argument("--base-time-shift", default=0.01, type=float,
                    help="The maximum time shift that can be caused by a single term in "
                         "the polynomial.")
parser.add_argument("--base-freq-shift", default=10., type=float,
                    help="The maximum frequency shift that can be caused by a single term in "
                         "the polynomial.")
parser.add_argument("--shift-param", default="mtotal", type=str,
                    help="The parameter used to calculate the time and frequency shifts")

# Gather inputs for output
parser.add_argument("--output-file", required=True,
                    help="The path of the output file to save the weights and losses")
parser.add_argument("--checkpoint", type=int, default=1,
                    help="If given save the weights after this many epochs")

# Gather additional options
parser.add_argument("--verbose", action='store_true')

args = parser.parse_args()

if args.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARNING
logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)

samples = SampleFile(args.sample_file)

template_mem = zeros(samples.tlen, dtype=complex64)

if args.enable_bank_start_frequency:
    low_frequency_cutoff = None
else:
    low_frequency_cutoff = args.low_frequency_cutoff

bank = waveform.FilterBank(args.bank_file, samples.flen, samples.delta_f,
                           low_frequency_cutoff=low_frequency_cutoff,
                           dtype=complex64, phase_order=args.order,
                           taper=args.taper_template, approximant=args.approximant,
                           out=template_mem, max_template_length=args.max_template_length)

freqs = np.arange(samples.flen) * samples.delta_f

min_param, max_param = samples.get_param_min_max(args.shift_param)
clip = lambda x: tf.clip_by_value(x, 1., args.shift_num)

if args.checkpoint_file is None:
    epoch = 0
    transform = PolyShiftTransform(args.shift_num, args.poly_degree,
                                   args.base_time_shift,
                                   args.base_freq_shift,
                                   max_param,
                                   freqs, samples.flow,
                                   samples.sample_rate // 2)

    threshold = tf.Variable(np.array([1.], dtype=np.float32), trainable=True,
                            dtype=tf.float32, constraint=clip)

else:
    with h5py.File(args.checkpoint_file, 'r') as f:
        epoch = f['loss_epoch'][-1]
        dfs = tf.convert_to_tensor(f['df'][-1, :, :])
        dts = tf.convert_to_tensor(f['dt'][-1, :, :])
        thresh = f['thresh'][-1, 0]
        opt_weights = [f['optimizer_weights_{0}'.format(i)][()]
                       for i in range(f.attrs['optimizer_weights_num'])]

    transform = PolyShiftTransform.from_weights(dts, dfs,
                                                args.base_time_shift,
                                                args.base_freq_shift,
                                                max_param,
                                                freqs, samples.flow,
                                                samples.sample_rate // 2)
    
    threshold = tf.Variable(np.array([thresh], dtype=np.float32), trainable=True,
                            dtype=tf.float32, constraint=clip)

chisq_transform = ChisqFilter(transform, threshold,
                              freqs, samples.flow, samples.sample_rate // 2)

train = transform.trainable_weights
train['thresh'] = threshold

trainable_names = [k for k in train.keys()]
trainable_names.sort()
trainable_weights = [train[k] for k in trainable_names]

optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)

if args.checkpoint_file is not None:
    grads = [tf.zeros_like(w) for w in trainable_weights]
    optimizer.apply_gradients(zip(grads, trainable_weights))
    optimizer.set_weights(opt_weights)

out_file = h5py.File(args.output_file, 'w')
out_file.attrs['shift_num'] = args.shift_num
out_file.attrs['poly_degree'] = args.poly_degree
out_file.attrs['base_time_shift'] = args.base_time_shift
out_file.attrs['base_freq_shift'] = args.base_freq_shift
out_file.attrs['shift_param'] = args.shift_param

loss_epochs = []
save_losses = []
check_epochs = []
save_checks = {t: [] for t in train.keys()}

for i in range(args.epochs):

    order = np.arange(samples.num)
    if args.shuffle:
        np.random.shuffle(order)

    nbatches = int(np.ceil(1. * samples.num / args.batch_size))

    losses = 0.

    inj_chis = 0.
    inj_count = 0
    trig_snrs = 0.
    trig_count = 0

    times = 0.

    for j in range(nbatches):

        lidx = int(1. * j * samples.num / nbatches)
        hidx = int(1. * (j + 1) * samples.num / nbatches)

        idxs = order[lidx:hidx]
        idxs = np.sort(idxs)

        logging.info("Starting batch")

        start = time.time()

        segs, psds, temp, injs, gather_idxs, param = samples.get_tensors(idxs, bank,
                                                                         args.shift_param)

        logging.info("Batch inputs read")

        with tf.GradientTape() as tape:

            snr_prime, chisq = chisq_transform.get_max_snr_prime(
                temp, segs, psds, param,
                gather_idxs=gather_idxs, max_snr=True
            )

            #trig_loss = - chi2(snr_prime ** 2., 2.)
            #inj_loss = - nc_chi2(snr_prime ** 2., 2., snr_max ** 2.)
            above = tf.greater(snr_prime, tf.ones_like(snr_prime) * 4.)
            trig_loss = tf.where(above, snr_prime - 4., tf.zeros_like(snr_prime))
            inj_loss = chisq ** 0.5

            batch_loss = tf.where(injs, inj_loss, trig_loss)
            loss = tf.reduce_mean(batch_loss)
            logging.info("Loss calculated")

        gradients = tape.gradient(loss, trainable_weights, unconnected_gradients='none')
        logging.info("Gradients calculated")

        optimizer.apply_gradients(zip(gradients, trainable_weights))
        logging.info("Gradients applied")
        logging.info("Completed batch")

        losses += loss

        inj_lgc = injs.numpy()
        trig_lgc = np.logical_not(inj_lgc)
        inj_chis += np.sum(batch_loss.numpy()[inj_lgc])
        inj_count += np.sum(inj_lgc)
        trig_snrs += np.sum(batch_loss.numpy()[trig_lgc])
        trig_count += np.sum(trig_lgc)

        times += time.time() - start

        if j == (nbatches - 1):
            end_str = '\n'
        else:
            end_str = '\r'
        print("Epoch: {0:5d}/{1:5d} Batch: {2:5d}/{3:5d}, ".format(i + 1, args.epochs, j + 1, nbatches)
              + "Av. Loss: {0:6.6f}, ".format(losses / (j + 1))
              + "Av. Inj Div: {0:6.6f}, ".format(inj_chis / inj_count)
              + "Av. Trig SNR > 4': {0:6.6f}".format(trig_snrs / trig_count)
              + "Av. Time: {0:6.2f}".format(times / (j + 1)),
              end=end_str)

    loss_epochs += [epoch + i + 1]
    save_losses += [losses / nbatches]

    if (i + 1) == args.epochs or (args.checkpoint and (i + 1) % args.checkpoint == 0):
        check_epochs += [epoch + i + 1]
        save_weights = transform.get_weights()
        for k, v in save_weights.items():
            save_checks[k] += [v.numpy()]
        save_checks['thresh'] += [threshold.numpy()]

_ = out_file.create_dataset('loss_epoch', data=np.array(loss_epochs))
_ = out_file.create_dataset('loss', data=np.array(save_losses))

_ = out_file.create_dataset('check_epoch', data=np.array(check_epochs))
for k, v in save_checks.items():
    _ = out_file.create_dataset(k, data=np.stack(v, axis=0))

opt_weights = optimizer.get_weights()
out_file.attrs['optimizer_weights_num'] = len(opt_weights)
for i in range(len(opt_weights)):
    opt = opt_weights[i]
    _ = out_file.create_dataset('optimizer_weights_{0}'.format(i), data=opt)

out_file.close()
