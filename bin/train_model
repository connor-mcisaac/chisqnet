#! /usr/bin/env python

import argparse
import logging, time
import h5py
import numpy as np
from pycbc import waveform
from pycbc.types import zeros, float32, complex64
from chisqnet.filtering import select_transformation, ChisqFilter
import tensorflow as tf
import tensorflow_probability as tfp
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from chisqnet.training_utils import SampleFile, chi2, nc_chi2


parser = argparse.ArgumentParser()

# Gather inputs
parser.add_argument("--training-sample-file", required=True,
                    help="Sample file to be used in training")
parser.add_argument("--validation-sample-file",
                    help="Sample file to be used in training")
parser.add_argument("--checkpoint-file",
                    help="Checkpoint file with the latest state of the "
                         "model to be loaded")

# Gather arguments for filtering/bank
parser.add_argument("--bank-file", required=True,
                    help="The bank file to be used to generate templates")
parser.add_argument("--low-frequency-cutoff", type=float,
                  help="The low frequency cutoff to use for filtering (Hz)")
parser.add_argument("--enable-bank-start-frequency", action='store_true',
                  help="Read the starting frequency of template waveforms"
                       " from the template bank.")
parser.add_argument("--max-template-length", type=float,
                  help="The maximum length of a template is seconds. The "
                       "starting frequency of the template is modified to "
                       "ensure the proper length")
waveform.bank.add_approximant_arg(parser)
parser.add_argument("--order", type=int,
                  help="The integer half-PN order at which to generate"
                       " the approximant. Default is -1 which indicates to use"
                       " approximant defined default.", default=-1,
                       choices = np.arange(-1, 9, 1))
taper_choices = ["start","end","startend"]
parser.add_argument("--taper-template", choices=taper_choices,
                    help="For time-domain approximants, taper the start and/or"
                         " end of the waveform before FFTing.")

# Gather arguments for training
parser.add_argument("--batch-size", required=True, type=int,
                    help="The number of samples to analyse in one batch")
parser.add_argument("--epochs", required=True, type=int,
                     help="The number of times to analyse the full dataset")
parser.add_argument("--shuffle", action="store_true",
                    help="If given, shuffle the order of analysed segments each epoch")
parser.add_argument("--snr-cut-width", default=0.1, type=float,
                    help="The width around each trigger to cut from the SNR timeseries")
parser.add_argument("--learning-rate", default=0.001, type=float,
                    help="The learning rate passed to the training optimizer")
parser.add_argument("--l1-weight", default=0.01, type=float,
                    help="The l1 regulariser weight")
parser.add_argument("--l2-weight", default=0.01, type=float,
                    help="The l2 regulariser weight")

# Gather arguments for transform
parser.add_argument("--transformation", required=True,
                    help="The type of transformation to be used")
parser.add_argument("--config-file", required=True,
                    help="The config gile used to create the transformation")
parser.add_argument("--threshold-max", required=True, type=float,
                    help="The maximum value the chisquared threshold can take")

# Gather inputs for output
parser.add_argument("--output-file", required=True,
                    help="The path of the output file to save the weights and losses")
parser.add_argument("--checkpoint", type=int, default=1,
                    help="If given save the weights after this many epochs")

# Gather additional options
parser.add_argument("--verbose", action='store_true')

args = parser.parse_args()

if args.verbose:
    log_level = logging.DEBUG
else:
    log_level = logging.WARNING
logging.basicConfig(format='%(asctime)s : %(message)s', level=log_level)

training_samples = SampleFile(args.training_sample_file)
if args.validation_sample_file:
    validation_samples = SampleFile(args.validation_sample_file)

template_mem = zeros(training_samples.tlen, dtype=complex64)

if args.enable_bank_start_frequency:
    low_frequency_cutoff = None
else:
    low_frequency_cutoff = args.low_frequency_cutoff

bank = waveform.FilterBank(
    args.bank_file, training_samples.flen, training_samples.delta_f,
    low_frequency_cutoff=low_frequency_cutoff,
    dtype=complex64, phase_order=args.order,
    taper=args.taper_template, approximant=args.approximant,
    out=template_mem, max_template_length=args.max_template_length
)

freqs = np.arange(training_samples.flen) * training_samples.delta_f

clip = lambda x: tf.clip_by_value(x, 1., args.threshold_max)

transform_class = select_transformation(args.transformation)

if args.checkpoint_file is None:
    epoch = 0
    transform = transform_class.from_config(
        args.config_file,
        freqs, training_samples.flow,
        training_samples.sample_rate // 2
    )

    threshold = tf.Variable(np.array([1.], dtype=np.float32), trainable=True,
                            dtype=tf.float32, constraint=clip)

else:
    with h5py.File(args.checkpoint_file, 'r') as f:
        epoch = f.attrs['epoch']
        thresh = f['thresh_epoch_{0}'.format(epoch)][0]
        g = f['optimizer_epoch_{0}'.format(epoch)]
        opt_weights = [g['weight_{0}'.format(i)][()]
                       for i in range(g.attrs['weights_num'])]

    transform = transform_class.from_file(
        args.checkpoint_file,
        freqs, training_samples.flow,
        training_samples.sample_rate // 2,
        group="model_epoch_{0}".format(epoch)
    )
    
    threshold = tf.Variable(np.array([thresh], dtype=np.float32), trainable=True,
                            dtype=tf.float32, constraint=clip)

chisq_transform = ChisqFilter(
    transform, threshold,
    freqs, training_samples.flow, training_samples.sample_rate // 2
)

train = transform.trainable_weights
train['thresh'] = threshold

trainable_names = [k for k in train.keys()]
trainable_names.sort()
trainable_weights = [train[k] for k in trainable_names]

optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate)

if args.checkpoint_file is not None:
    grads = [tf.zeros_like(w) for w in trainable_weights]
    optimizer.apply_gradients(zip(grads, trainable_weights))
    optimizer.set_weights(opt_weights)

output_file_created = False

for i in range(args.epochs):

    logging.info("Starting epoch")
    logging.info("Starting training")

    order = np.arange(training_samples.num)
    if args.shuffle:
        np.random.shuffle(order)

    nbatches = int(np.ceil(1. * training_samples.num / args.batch_size))

    losses = []

    inj_losses = 0.
    inj_count = 0
    trig_losses = 0.
    trig_count = 0

    times = 0.

    for j in range(nbatches):

        lidx = int(1. * j * training_samples.num / nbatches)
        hidx = int(1. * (j + 1) * training_samples.num / nbatches)

        idxs = order[lidx:hidx]
        idxs = np.sort(idxs)

        logging.info("Starting batch")

        start = time.time()

        segs, psds, temp, injs, gather_idxs, params = \
            training_samples.get_tensors(idxs, bank)

        logging.info("Batch inputs read")

        with tf.GradientTape() as tape:

            snr_prime, chisq = chisq_transform.get_max_snr_prime(
                temp, segs, psds, params,
                gather_idxs=gather_idxs,
                max_snr=True, training=True
            )

            snr = snr_prime * (chisq ** 0.5)
            trig_loss = (tf.maximum(snr_prime, tf.ones_like(snr_prime) * 4.) - 4.) ** 2.
            inj_loss = (snr - snr_prime) ** 2.

            batch_loss = tf.where(injs, inj_loss, trig_loss)
            loss = tf.reduce_mean(batch_loss)

            if args.l1_weight or args.l2_weight:
                l1_l2_losses = []
                for n, w in zip(trainable_names, trainable_weights):
                    if n == 'thresh':
                        continue
                    l1_loss = 0.
                    l2_loss = 0.
                    if args.l1_weight:
                        l1_loss = args.l1_weight * tf.reduce_sum(tf.math.abs(w)) / nbatches
                    if args.l2_weight:
                        l2_loss = args.l2_weight * tf.reduce_sum(tf.math.square(w)) / nbatches
                    l1_l2_losses.append(l1_loss + l2_loss)
                l1_l2_loss = tf.math.add_n(l1_l2_losses)
                loss += l1_l2_loss
                    
            logging.info("Loss calculated")

        gradients = tape.gradient(loss, trainable_weights, unconnected_gradients='none')
        logging.info("Gradients calculated")

        optimizer.apply_gradients(zip(gradients, trainable_weights))
        logging.info("Gradients applied")
        logging.info("Completed batch")

        losses += [loss]

        inj_lgc = injs.numpy()
        trig_lgc = np.logical_not(inj_lgc)
        inj_losses += np.sum(batch_loss.numpy()[inj_lgc])
        inj_count += np.sum(inj_lgc)
        trig_losses += np.sum(batch_loss.numpy()[trig_lgc])
        trig_count += np.sum(trig_lgc)

        times += time.time() - start

        if j == (nbatches - 1):
            end_str = '\n'
        else:
            end_str = '\r'

        print("Epoch: {0:5d}/{1:5d} ".format(i + 1, args.epochs)
              + "Batch: {0:5d}/{1:5d}, ".format(j + 1, nbatches)
              + "Av. Loss: {0:6.6f}, ".format(np.sum(losses) / (j + 1))
              + "Av. Inj Loss: {0:6.6f}, ".format(inj_losses / inj_count)
              + "Av. Trig Loss: {0:6.6f}, ".format(trig_losses / trig_count)
              + "Av. Time: {0:6.2f}".format(times / (j + 1)),
              end=end_str)

    if (i + 1) == args.epochs or (args.checkpoint and (i + 1) % args.checkpoint == 0):
        transform.to_file(
            args.output_file,
            group="model_epoch_{0}".format(epoch + i + 1),
            append=output_file_created
        )

        opt_weights = optimizer.get_weights()
        with h5py.File(args.output_file, 'a') as f:
            g = f.create_group('optimizer_epoch_{0}'.format(epoch + i + 1))
            g.attrs['weights_num'] = len(opt_weights)
            for j in range(len(opt_weights)):
                opt = opt_weights[j]
                _ = g.create_dataset('weight_{0}'.format(j), data=opt)
            _ = f.create_dataset('training_loss_epoch_{0}'.format(epoch + i + 1), data=np.array(losses))
            _ = f.create_dataset('thresh_epoch_{0}'.format(epoch + i + 1), data=threshold.numpy())

        output_file_created = True

    if args.validation_sample_file is None:
        continue

    logging.info("Starting validation")
    order = np.arange(validation_samples.num)
    nbatches = int(np.ceil(1. * validation_samples.num / args.batch_size))

    losses = []

    inj_losses = 0.
    inj_count = 0
    trig_losses = 0.
    trig_count = 0

    times = 0.

    for j in range(nbatches):

        lidx = int(1. * j * validation_samples.num / nbatches)
        hidx = int(1. * (j + 1) * validation_samples.num / nbatches)

        idxs = order[lidx:hidx]
        idxs = np.sort(idxs)

        logging.info("Starting batch")

        start = time.time()

        segs, psds, temp, injs, gather_idxs, params = \
            validation_samples.get_tensors(idxs, bank)

        logging.info("Batch inputs read")

        snr_prime, chisq = chisq_transform.get_max_snr_prime(
            temp, segs, psds, params,
            gather_idxs=gather_idxs,
            max_snr=True, training=False
        )

        snr = snr_prime * (chisq ** 0.5)
        trig_loss = (tf.maximum(snr_prime, tf.ones_like(snr_prime) * 4.) - 4.) ** 2.
        inj_loss = (snr - snr_prime) ** 2.

        batch_loss = tf.where(injs, inj_loss, trig_loss)
        loss = tf.reduce_mean(batch_loss)

        logging.info("Loss calculated")
        logging.info("Completed batch")

        losses += [loss]

        inj_lgc = injs.numpy()
        trig_lgc = np.logical_not(inj_lgc)
        inj_losses += np.sum(batch_loss.numpy()[inj_lgc])
        inj_count += np.sum(inj_lgc)
        trig_losses += np.sum(batch_loss.numpy()[trig_lgc])
        trig_count += np.sum(trig_lgc)

        times += time.time() - start

        if j == (nbatches - 1):
            end_str = '\n'
        else:
            end_str = '\r'

        print("Epoch: {0:5d}/{1:5d} ".format(i + 1, args.epochs)
              + "Batch: {0:5d}/{1:5d}, ".format(j + 1, nbatches)
              + "Av. Loss: {0:6.6f}, ".format(np.sum(losses) / (j + 1))
              + "Av. Inj Loss: {0:6.6f}, ".format(inj_losses / inj_count)
              + "Av. Trig Loss: {0:6.6f}, ".format(trig_losses / trig_count)
              + "Av. Time: {0:6.2f}".format(times / (j + 1)),
              end=end_str)

        with h5py.File(args.output_file, 'a') as f:
            _ = f.create_dataset('validation_loss_epoch_{0}'.format(epoch + i + 1), data=np.array(losses))
