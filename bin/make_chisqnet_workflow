#!/usr/bin/env python

"""
Script for creating a workflow to prepare a training set and train a model
for chi-squared discriminators.
"""

import os, argparse, logging, sys, socket, numpy
import pycbc.workflow as wf
from Pegasus import DAX3 as dax
from pycbc.results import layout, create_versioning_page
from pycbc.results.versioning import save_fig_with_metadata
from ligo import segments
from pycbc.workflow import PlotExecutable
from chisqnet.workflow import (FilterTriggersExecutable, PreprocessingDaxGenerator,
                               MergeSamplesExecutable, StageoutExecutable,
                               PrepareSamplesExecutable, TrainingExecutable,
                               MergeModelsExecutable)


# Log to the screen until we know where the output file is
logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
    level=logging.INFO)

parser = argparse.ArgumentParser(description=__doc__[1:])
#parser.add_argument('--version', action='version', version=__version__)
parser.add_argument('--workflow-name', default='my_unamed_run')
parser.add_argument("-d", "--output-dir", default=None,
                    help="Path to output directory.")
wf.add_workflow_command_line_group(parser)
args = parser.parse_args()

container = wf.Workflow(args, args.workflow_name)
workflow = wf.Workflow(args, args.workflow_name + '-main')
finalize_workflow = wf.Workflow(args, args.workflow_name + '-finalization')

wf.makedir(args.output_dir)
os.chdir(args.output_dir)
args.output_dir = '.'
wf.makedir('daxes')

rdir = layout.SectionNumber('results', ['samples',
                                        'training',
                                        'trained_model',
                                        'workflow'])
wf.makedir(rdir.base)
wf.makedir(rdir['workflow'])

wf_log_file = wf.File(workflow.ifos, 'workflow-log', workflow.analysis_time,
                      extension='.txt',
                      directory=rdir['workflow'])

logging.basicConfig(format='%(asctime)s:%(levelname)s : %(message)s',
                    filename=wf_log_file.storage_path,
                    level=logging.INFO,
                    filemode='w')

logfile = logging.FileHandler(filename=wf_log_file.storage_path,mode='w')
logfile.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s:%(levelname)s : %(message)s')
logfile.setFormatter(formatter)
logging.getLogger('').addHandler(logfile)
logging.info("Created log file %s" % wf_log_file.storage_path)

# Gather input files from config and setup filtering jobs and preprocessing
logging.info("Setting up trigger filtering and preprocessing")
filter_job = FilterTriggersExecutable(workflow.cp, 'filter', out_dir='filtered_triggers',
                                      ifos=workflow.ifos, tags=[])

sample_files = wf.FileList([])
sub_dax_jobs = []

subsections = workflow.cp.get_subsections('filter')
for section in subsections:
    logging.info("Creating job for {0}".format(section))
    curr_tags = [section.upper()]

    start_time = workflow.cp.get_opt_tags("workflow-filter", "start-time", curr_tags)
    end_time = workflow.cp.get_opt_tags("workflow-filter", "end-time", curr_tags)
    seg = segments.segment([int(start_time), int(end_time)])

    file_attrs = {'ifos': workflow.ifos,
                  'segs': seg,
                  'tags': curr_tags}

    # Select single detector trigger files
    single_det_files = wf.FileList([])
    for ifo in workflow.ifos:
        filestr = workflow.cp.get_opt_tags("workflow-filter", ifo.lower() + "-trigger-file", curr_tags)
        single_det_file = wf.resolve_url_to_file(filestr, attrs=file_attrs)
        single_det_files.append(single_det_file)

    # Select bank file
    filestr = workflow.cp.get_opt_tags("workflow-filter", "bank-file", curr_tags)
    bank_file = wf.resolve_url_to_file(filestr, attrs=file_attrs)

    # Select veto file if given
    if workflow.cp.has_option_tags("workflow-filter", "veto-file", curr_tags):
        filestr = workflow.cp.get_opt_tags("workflow-filter", "veto-file", curr_tags)
        veto_file = wf.resolve_url_to_file(filestr, attrs=file_attrs)
    else:
        veto_file = None

    # Select injection and injfind files if they are given
    injfind = workflow.cp.has_option_tags("workflow-filter", "injfind-file", curr_tags)
    injection = workflow.cp.has_option_tags("workflow-filter", "injection-file", curr_tags)

    if injfind and injection:
        injfind_filestr = workflow.cp.get_opt_tags("workflow-filter", "injfind-file", curr_tags)
        injfind_file = wf.resolve_url_to_file(injfind_filestr, attrs=file_attrs)

        injection_filestr = workflow.cp.get_opt_tags("workflow-filter", "injection-file", curr_tags)
        injection_file = wf.resolve_url_to_file(injection_filestr, attrs=file_attrs)
    elif injfind or injection:
        raise ValueError("Both injfind-file and injection-file must be given or neither")
    else:
        injfind_file = None
        injection_file = None

    # Setup node using filter executable
    filter_job.update_current_tags(curr_tags)
    filter_node = filter_job.create_node(single_det_files, bank_file, injfind_file, veto_file)
    workflow.add_node(filter_node)
    trigger_file = filter_node.output_files[0]

    # Setup Preprocessing sub-workflow
    config_path = os.path.abspath('daxes' + '/' + 'preprocessing_' + section + '.ini')
    workflow.cp.write(open(config_path, 'w'))
    config_file = wf.File.from_path(config_path)

    preprocessing_exe = PreprocessingDaxGenerator(workflow.cp, 'preprocessing',
                                                  ifos=workflow.ifos, out_dir='daxes')

    preprocessing_out_dir = os.path.abspath('preprocessing_' + section)
    sample_file_sub = wf.File(preprocessing_exe.ifo_list, preprocessing_exe.name,
                              seg, extension='.hdf', store_file=True,
                              directory=preprocessing_out_dir,
                              tags=curr_tags+['SAMPLES', 'SUB'],
                              use_tmp_subdirs=False)

    sub_workflow_name = 'preprocessing_' + section + '_workflow'
    preprocessing_node = preprocessing_exe.create_node(seg, config_file,
                                                       subsections,
                                                       preprocessing_out_dir,
                                                       sample_file_sub,
                                                       trigger_file, injection_file,
                                                       workflow_name=sub_workflow_name,
                                                       sub_wf_tags=curr_tags)

    workflow.add_node(preprocessing_node)

    dax_file = preprocessing_node.output_files[0]
    map_file = preprocessing_node.output_files[1]
    tc_file = preprocessing_node.output_files[2]
    dax_job = dax.DAX(dax_file)
    dax_job.addArguments('--basename %s' % \
                         os.path.splitext(os.path.basename(dax_file.name))[0])
    wf.Workflow.set_job_properties(dax_job, map_file, tc_file)

    #dax_job.uses(trigger_file, link=dax.Link.INPUT, register=False, transfer=True)
    #if injection_file is not None:
    #    dax_job.uses(injection_file, link=dax.Link.INPUT, register=False, transfer=True)

    workflow._adag.addJob(dax_job)
    dep = dax.Dependency(parent=preprocessing_node._dax_node, child=dax_job)
    workflow._adag.addDependency(dep)
    sub_dax_jobs.append(dax_job)

    # In order to use the samples file in the workflow we pegasus must know how it is generated.
    # Registering samples_file_sub as an output file will add it to the input catalogue and it
    # will never be generated.
    # Instead copy it to a new file and make the copy job dependant on the sub-dax.
    sample_file = wf.File(preprocessing_exe.ifo_list, preprocessing_exe.name,
                          seg, extension='.hdf', store_file=True,
                          directory='training_samples',
                          tags=curr_tags+['SAMPLES'], use_tmp_subdirs=False)
    
    stageout_exe = StageoutExecutable(workflow.cp, 'manual-stageout', out_dir=args.output_dir,
                                      ifos=workflow.ifos, tags=curr_tags+['PREPROCESSING'])
    node = stageout_exe.create_node(sample_file_sub, sample_file)
    workflow.add_node(node)

    dep = dax.Dependency(parent=dax_job, child=node._dax_node)
    workflow._adag.addDependency(dep)

    sample_files.append(sample_file)

    # Plot distribution of samples
    plots = wf.FileList([])

    table_job = PlotExecutable(workflow.cp, 'page-sample-table',
                               out_dir=rdir['samples/{0}_samples'.format(section)],
                               ifos=workflow.ifos, tags=curr_tags)
    node = table_job.create_node()
    node.add_input_opt('--sample-file', sample_file)
    node.add_list_opt('--ifos', workflow.ifos)
    node.new_output_file_opt(sample_file.segment, '.html', '--output-file')
    workflow.add_node(node)
    plots.append(node.output_files[0])

    secs = workflow.cp.get_subsections('plot-samples')
    for sec in secs:
        for ifo in workflow.ifos:
            sample_job = PlotExecutable(workflow.cp, 'plot-samples',
                                        out_dir=rdir['samples/{0}_samples'.format(section)],
                                        ifos=ifo, tags=[sec] + curr_tags)
            node = sample_job.create_node()
            node.add_input_opt('--single-trig-file', sample_file)
            node.add_input_opt('--bank-file', bank_file)
            node.add_opt('--detector', ifo)
            node.new_output_file_opt(sample_file.segment, '.png', '--output-file')
            workflow.add_node(node)
            plots.append(node.output_files[0])
    layout.single_layout(rdir['samples/{0}_samples'.format(section)], plots)

logging.info("Trigger filtering and preprocessing setup")

# Setup job to merge samples
logging.info("Setting up sample merging job")
merge_job = MergeSamplesExecutable(workflow.cp, 'merge-samples', out_dir='training_samples',
                                   ifos=workflow.ifos, tags=['ALL'])
merge_job.update_current_retention_level(wf.Executable.MERGED_TRIGGERS)
node = merge_job.create_node(sample_files, create_bank=True)
workflow.add_node(node)
all_samples = node.output_files[0]
bank_file = node.output_files[1]

# Plot distribution of samples
plots = wf.FileList([])

table_job = PlotExecutable(workflow.cp, 'page-sample-table',
                           out_dir=rdir['samples'],
                           ifos=workflow.ifos, tags=['ALL'])
node = table_job.create_node()
node.add_input_opt('--sample-file', all_samples)
node.add_list_opt('--ifos', workflow.ifos)
node.new_output_file_opt(all_samples.segment, '.html', '--output-file')
workflow.add_node(node)
plots.append(node.output_files[0])

secs = workflow.cp.get_subsections('plot-samples')
for sec in secs:
    for ifo in workflow.ifos:
        sample_job = PlotExecutable(workflow.cp, 'plot-samples',
                                    out_dir=rdir['samples'],
                                    ifos=ifo, tags=[sec])
        node = sample_job.create_node()
        node.add_input_opt('--single-trig-file', all_samples)
        node.add_input_opt('--bank-file', bank_file)
        node.add_opt('--detector', ifo)
        node.new_output_file_opt(all_samples.segment, '.png', '--output-file')
        workflow.add_node(node)
        plots.append(node.output_files[0])
layout.single_layout(rdir['samples'], plots)


# Setup job to split and flatten sample
logging.info("Setting up sample preperation job")
merge_job = PrepareSamplesExecutable(workflow.cp, 'prepare-samples', out_dir='training_samples',
                                     ifos=workflow.ifos, tags=[])
merge_job.update_current_retention_level(wf.Executable.MERGED_TRIGGERS)
node = merge_job.create_node(all_samples)
workflow.add_node(node)
training_samples = node.output_files[0]
validation_samples = node.output_files[1]

# Setup training and validation jobs
file_attrs = {'ifos': workflow.ifos,
              'segs': training_samples.segment,
              'tags': []}
filestr = workflow.cp.get_opt_tags("workflow-train-model", "config-file", [])
training_config_file = wf.resolve_url_to_file(filestr, attrs=file_attrs)

epochs_total = int(workflow.cp.get_opt_tags("workflow-train-model", "epochs-total", []))
epochs_per = int(workflow.cp.get_opt_tags("workflow-train-model", "epochs-per-job", []))
epochs_per = min(epochs_total, epochs_per)

logging.info("Setting up training jobs")
training_job = TrainingExecutable(workflow.cp, 'train-model', out_dir='trained_model',
                                  ifos=workflow.ifos, tags=['EPOCHS', '0', str(epochs_per)])
node = training_job.create_node(epochs_per, training_samples, validation_samples,
                                bank_file, training_config_file)
workflow.add_node(node)
checkpoint_files = [node.output_files[0]]

epochs_done = epochs_per
while epochs_done < epochs_total:
    epochs_left = epochs_total - epochs_done
    epochs_next = min(epochs_per, epochs_left)

    training_job = TrainingExecutable(workflow.cp, 'train-model', out_dir='trained_model',
                                      ifos=workflow.ifos,
                                      tags=['EPOCHS', str(epochs_done), str(epochs_next)])
    node = training_job.create_node(epochs_next, training_samples, validation_samples,
                                    bank_file, training_config_file,
                                    checkpoint=checkpoint_files[-1])
    workflow.add_node(node)
    checkpoint_files += [node.output_files[0]]

    epochs_done += epochs_next

logging.info("Training job setup")

# Merge training and validation files
logging.info("Setting up output file merging")
merge_job = MergeModelsExecutable(workflow.cp, 'merge-models', out_dir='trained_model',
                                  ifos=workflow.ifos, tags=[])
node = merge_job.create_node(wf.FileList(checkpoint_files))
workflow.add_node(node)
trained_model = node.output_files[0]

# Setup plotting jobs
logging.info("Setting up plotting jobs")

loss_job = PlotExecutable(workflow.cp, 'plot-loss', out_dir=rdir['training'],
                          ifos=workflow.ifos, tags=[])
node = loss_job.create_node()
node.add_input_opt('--input-file', trained_model)
node.new_output_file_opt(trained_model.segment, '.png', '--output-file')
workflow.add_node(node)
layout.two_column_layout(rdir['training'], [(node.output_files[0],)])

shift_job = PlotExecutable(workflow.cp, 'plot-model', out_dir=rdir['trained_model'],
                           ifos=workflow.ifos, tags=[])
node = shift_job.create_node()
node.add_input_opt('--model-file', trained_model)
node.add_input_opt('--bank-file', bank_file)
node.new_output_file_opt(trained_model.segment, '.png', '--output-file')
workflow.add_node(node)
layout.two_column_layout(rdir['trained_model'], [(node.output_files[0],)])

# Create results pages
logging.info("Setting up results pages")
create_versioning_page(rdir['workflow/version'], container.cp)

base = rdir['workflow/configuration']
wf.makedir(base)
ini_file_path = os.path.join(base, 'configuration.ini')
with open(ini_file_path, 'w') as ini_fh:
    container.cp.write(ini_fh)
ini_file = wf.FileList([wf.File(workflow.ifos, '', trained_model.segment,
                        file_url='file://' + ini_file_path)])
layout.single_layout(base, ini_file)

log_file_html = wf.File(workflow.ifos, 'WORKFLOW-LOG', trained_model.segment,
                        extension='.html', directory=rdir['workflow'])

dashboard_file = wf.File(workflow.ifos, 'DASHBOARD', trained_model.segment,
                         extension='.html', directory=rdir['workflow'])
dashboard_str = """<center><p style="font-size:20px"><b><a href="PEGASUS_DASHBOARD_URL" target="_blank">Pegasus Dashboard Page</a></b></p></center>"""
kwds = { 'title' : "Pegasus Dashboard",
         'caption' : "Link to Pegasus Dashboard",
         'cmd' : "PYCBC_SUBMIT_DAX_ARGV", }
save_fig_with_metadata(dashboard_str, dashboard_file.storage_path, **kwds)

wf.makedir(rdir['workflow/dax'])
wf.makedir(rdir['workflow/input_map'])
wf.makedir(rdir['workflow/output_map'])
wf.makedir(rdir['workflow/planning'])

wf.make_results_web_page(finalize_workflow, os.path.join(os.getcwd(), rdir.base))

container += workflow
container += finalize_workflow

dep = dax.Dependency(parent=workflow.as_job, child=finalize_workflow.as_job)
container._adag.addDependency(dep)

container.save()

logging.info("Written dax.")

logging.shutdown()
with open (wf_log_file.storage_path, "r") as logfile:
    logdata=logfile.read()
log_str = """
<p>Workflow generation script created workflow in output directory: %s</p>
<p>Workflow name is: %s</p>
<p>Workflow generation script run on host: %s</p>
<pre>%s</pre>
""" % (os.getcwd(), args.workflow_name, socket.gethostname(), logdata)
kwds = { 'title' : 'Workflow Generation Log',
         'caption' : "Log of the workflow script %s" % sys.argv[0],
         'cmd' :' '.join(sys.argv), }
save_fig_with_metadata(log_str, log_file_html.storage_path, **kwds)
layout.single_layout(rdir['workflow'], ([dashboard_file,log_file_html]))
